# ğŸ“˜ Mini-Projet dâ€™Optimisation pour lâ€™Apprentissage Automatique

## ğŸ‘¤ Informations gÃ©nÃ©rales

- **RÃ©alisÃ© par** : Mohamed Lemine Abdallahi Tah  
- **Matricule** : C12896  
- **UniversitÃ©** : UniversitÃ© de Nouakchott Al Aasriya  
- **FacultÃ©** : FacultÃ© des Sciences et Techniques (FST)  
- **DÃ©partement** : MathÃ©matiques et Informatiques  
- **Niveau** : M2 SSD â€“ Statistiques et Sciences des DonnÃ©es  
- **AnnÃ©e universitaire** : 2025â€“2026  
- **Encadrant** : Dr Elbenany Med Mahmoud  

---

## ğŸ¯ Objectif du projet

Ce mini-projet a pour objectif dâ€™appliquer les **mÃ©thodes dâ€™optimisation vues en cours** Ã  des problÃ¨mes concrets dâ€™apprentissage automatique, en mettant lâ€™accent sur :

- la formalisation mathÃ©matique rigoureuse,
- lâ€™implÃ©mentation manuelle des algorithmes,
- lâ€™analyse thÃ©orique (convexitÃ©, LipschitzianitÃ©, convergence),
- la comparaison expÃ©rimentale des mÃ©thodes.

Aucun framework de deep learning (PyTorch, TensorFlow, etc.) nâ€™est utilisÃ© afin de mieux comprendre les mÃ©canismes fondamentaux de lâ€™optimisation.

---

## ğŸ“š Concepts et mÃ©thodes Ã©tudiÃ©s

### ğŸ”¹ Fonction de perte
- Perte logistique pour la classification binaire
- RÃ©gularisation **L2 (Ridge)** et **L1 (Lasso)**

### ğŸ”¹ PropriÃ©tÃ©s thÃ©oriques
- ConvexitÃ© et forte convexitÃ©  
- Gradient **L-Lipschitz continu**  
- StabilitÃ© numÃ©rique aprÃ¨s normalisation  

### ğŸ”¹ MÃ©thodes dâ€™optimisation

#### MÃ©thodes dÃ©terministes
- Descente de Gradient (GD)
- Gradient ConjuguÃ© (CG)

#### MÃ©thodes stochastiques
- Stochastic Gradient Descent (SGD) avec pas dÃ©croissant
- RMSProp
- Adam (analyse du momentum)

#### MÃ©thodes proximales (L1)
- ISTA
- FISTA

---

## ğŸ“Š Datasets utilisÃ©s

- **Breast Cancer Wisconsin** (classification binaire)  
  `sklearn.datasets.load_breast_cancer()`

- **California Housing** (rÃ©gression)  
  `sklearn.datasets.fetch_california_housing()`

Tous les datasets sont chargÃ©s directement depuis `scikit-learn`.

---

## ğŸ› ï¸ Technologies et outils

- **Langage** : Python 3  
- **BibliothÃ¨ques** :
  - NumPy (calcul matriciel, gradients)
  - Matplotlib (visualisation)
  - scikit-learn (datasets, normalisation, mÃ©triques)
- **Environnement** : Jupyter Notebook  
- **RÃ©daction du rapport** : LaTeX (Overleaf)

â¡ï¸ Aucune dÃ©pendance externe supplÃ©mentaire nâ€™est requise.

---

## ğŸ§ª MÃ©thodologie

Le projet suit strictement les phases dÃ©finies dans lâ€™Ã©noncÃ© officiel :

### Phase 1 â€“ Optimisation dÃ©terministe
- Analyse thÃ©orique de la perte logistique Ridge
- ImplÃ©mentation et comparaison de GD et CG

### Phase 2 â€“ Optimisation stochastique
- Passage Ã  lâ€™Ã©chelle avec SGD, RMSProp et Adam
- Analyse de la convergence et de la stabilitÃ©

### Phase 3 â€“ MÃ©thodes proximales
- RÃ©gularisation L1
- Comparaison ISTA vs FISTA
- Ã‰tude de la sparsitÃ© des coefficients

ğŸ“Œ Les donnÃ©es sont **normalisÃ©es avec `StandardScaler`**, garantissant :
- une meilleure stabilitÃ© numÃ©rique,
- un gradient Lipschitzien avec **L â‰ˆ 3.42**,
- une convergence rapide.

---

## ğŸ“ˆ RÃ©sultats principaux

AprÃ¨s normalisation :

- **GD et CG**
  - Convergence trÃ¨s rapide (< 0.1 s)
  - Accuracy â‰ˆ **96.5 %**, AUC â‰ˆ **0.996**

- **Adam et RMSProp**
  - Excellentes performances initiales
  - Adam lÃ©gÃ¨rement supÃ©rieur en stabilitÃ©

- **SGD**
  - TrÃ¨s compÃ©titif malgrÃ© sa simplicitÃ©

- **ISTA vs FISTA**
  - FISTA converge nettement plus vite

- **RÃ©gularisation L1**
  - Forte sparsitÃ© (80â€“100 % de coefficients nuls pour Î» â‰¥ 0.1)
  - LÃ©gÃ¨re baisse des performances prÃ©dictives

ğŸ‘‰ La normalisation est indispensable pour Ã©viter les instabilitÃ©s numÃ©riques.

---

## ğŸ“ Structure du dÃ©pÃ´t

```text
Optimization/
â”œâ”€â”€ projet/
â”‚   â”œâ”€â”€ figures/                  # Figures de convergence et comparaisons
â”‚   â”œâ”€â”€ optimizationML.ipynb      # Notebook principal
â”‚   â”œâ”€â”€ optimizationML.pdf        # Rapport final
â”‚   â””â”€â”€ sujetProjet.pdf           # Ã‰noncÃ© officiel
â”œâ”€â”€ tp/
â”‚   â””â”€â”€ tp.ipynb                  # TP complÃ©mentaire (rÃ©gression)
â””â”€â”€ README.md                     # Ce fichier
